\part{Learning Systems}
\thispagestyle{empty}

%\input{commands}

\title{Introduction}
\label{chp:into-learning-systems}
\author{Amanda Cristina Fraga De Albuquerque\\
        Brendon Erick Euzebio Rus Peres\\
        Erikson Freitas de Morais\\
        Gilson Junior Soares\\
        Jose Lohame Capinga\\
        Marcella Scoczynski Ribeiro Martins}
\institute{Federal University of Technology - Parana, Brazil}
\authorrunning{Amanda Cristina Fraga De Albuquerque et al.}

%\begin{document}
\maketitle

Early Artificial Intelligence (AI) studies tried and solved many problems that were considered difficult for humans but relatively easy for computers \cite{goodfellow2016}. These were problems that could be formally described using mathematical rules. 
Over time, we began to realize that the difficulty did not necessarily reside in these problems, but in those that are easily, even instinctively and intuitively carried out by humans, such as recognizing familiar faces, understanding languages, etc. The point is that human beings, on a daily basis, receive and process huge amounts of information and trying to make computers perform these activities only with pre-defined rules described by us was not viable. Many researchers started to develop techniques where the computer itself, through algorithms, learns to abstract these rules and information on its own by learning from data. This subfield of AI is called Machine Learning. This part of the book is going to introduce several machine learning algorithms.

%Today, as we hear a lot about AI or machine learning, we tend to think this is a recent area, but it has actually existed for many years. For example, one the the popular machine learning techniques uses a model inspired by biological brains, containing neurons and connections known as Neural Networks. However,  the idea of making computers mimic the brain's working scheme dates back to 1943, when Warren McCulloch and Walter Pitts suggested the idea in his article “A logical calculus of the ideas immanent in nervous activity” \cite{mcculloch1943}. There are also many other types of machine learning techniques that are not necessarily inspired by the biological brain. In recent years, we have seen an ever-increasing range of applications for machine learning techniques, partly due to the ever increasing amounts of data from various domains available for computers to learn from. 

Two main categories of machine learning algorithms are \textit{supervised} and \textit{unsupervised} learning algorithms. In supervised learning, algorithms have access to data in the form of input-output pairs. The task is to use such data to learn how to predict the outputs given the inputs. For example, one may be interested in predicting whether a patient has cancer (output variable) based on variables describing symptoms that this patient may or may not be displaying (input variables). Or, one may wish to predict the value of a given stock market (output variable) based on its values over the past 5 days (input variables). The function learned by the machine learning algorithm should work well not only for predicting the outputs of previously available data, but also for predicting the outputs of previously unseen inputs. In other words, supervised learning algorithms aim to learn functions able to \textit{generalize} to unseen data. For instance, a function to predict whether patients do or do not have cancer should work well not only for past patients (for which we already know whether or not they have cancer), but also future patients (for which we do not yet know whether they do or do not have cancer). Similarly, a function to predict the value of the stock market should work not only for past days (for which the stock market value is already known), but also for future days (for which the value of the stock is still unknown).

We can formalize supervised learning as follows: consider a set of examples 

\[\mathcal{T} = \{(\mathbf{x}^{(1)},\mathbf{y}^{(1)}),(\mathbf{x}^{(2)},\mathbf{y}^{(2)}),\cdots,(\mathbf{x}^{(N)},\mathbf{y}^{(N)})\},\]

\noindent where $(\mathbf{x}^{(i)},\mathbf{y}^{(i)}) \in \mathcal{X} \times \mathcal{Y}$  are examples drawn i.i.d. (independently and identically distributed) from a fixed albeit unknown joint probability distribution $p(\mathbf{x},\mathbf{y})$, $(x_1^{(i)},x_2^{(i)},\cdots,x_d^{(i)})^T$ is the vector of input variables of example $i$, $d$ is the dimensionality of the inputs of the problem, $\mathbf{y}^{(i)} = (y_1^{(i)}, y_2^{(i)}, \cdots, y_{d'}^{(i)})^T$ is the output variable of example $i$, $d'$ is the dimensionality of the outputs of the problem, and the symbol $T$ indicates the transpose of a vector. Many problems have a single output variable (i.e.,  $d'=1$), in which case we express $\mathbf{y}^{(i)}$ directly as a scalar $y^{(i)}$. When $\mathcal{Y}$ is the set of real values, the problem is called a regression problem. When it is a set of categories, it is called a classification problem. 

Supervised learning aims at using $\mathcal{T}$ to learn a function $f: \mathcal{X} \rightarrow \mathcal{Y}$ able to predict the output values $\mathbf{y} \in \mathcal{Y}$ corresponding to any input $\mathbf{x} \in \mathcal{X}$. 
This function can be referred to as a  predictive model or hypothesis. When dealing with classification problems, it is also common to refer to this function as a classifier.
As the data set $\mathcal{T}$ is used to learn this function, it is referred to as the \textit{training set}. Similarly, the examples in this set are referred to as \textit{training examples}. Typically, the learning of $f$ assumes that new (previously unseen) data also follows the joint probability distribution $p(\mathbf{x},\mathbf{y})$, though some advanced techniques also exist to deal with situations where this may not be the case. When the learned function $f$ is able to predict the outputs of unseen examples very well, it is said that this function \textit{generalizes} well to unseen data. 

When dealing with real world problems, it is usually inevitable that the learned function $f(\mathbf{x})$ will make some mistakes, i.e., it will not be able to always correctly predict the output values  $\mathbf{y} \in \mathcal{Y}$ corresponding to any input $\mathbf{x} \in \mathcal{X}$. Quite often, when the learning process attempts to create a function $f$ able to perfectly predict the outputs of the examples in the training set $\mathcal{T}$, this model becomes unable to generalize well to unseen examples. This is because the training set $\mathcal{T}$ frequently contains examples with \textit{noise}. These are examples that contain some atypical values for their input or output variables as a result of possible errors in the data collection. When a learning algorithm attempts to predict the outputs of all training examples perfectly, it may end up incorporating such noise, resulting in poor predictions on unseen data. When this happens, it is said that $f$ is overfitting the training examples. Machine learning algorithms thus typically use strategies to avoid overfitting, so that the learned functions $f$ can make the fewest possible mistakes on unseen data.  

It is worth noting that, sometimes, the input variables of all training examples are placed in a matrix $\mathbf{X}$ referred to as the design matrix:

\begin{align}
    \mathbf{X} &= \begin{pmatrix}
           x^{(1)}_{1} & x^{(1)}_{2} & \cdots & x^{(1)}_{d} \\
           x^{(2)}_{1} & x^{(2)}_{2} & \cdots & x^{(2)}_{d} \\
           \vdots & \vdots & \ddots & \vdots \\
		   x^{(N)}_{1} & x^{(N)}_{2} & \cdots & x^{(N)}_{d} \\
         \end{pmatrix}
  \end{align}

Correspondingly, the output variables corresponding to each training example can be placed in an output matrix. When the dimensionality of the outputs of the problem is $d'=1$, a vector containing the outputs of all training examples can be used instead:

\begin{align}
    \mathbf{y} &= \begin{pmatrix}
           x^{(1)} \\
           x^{(2)} \\
           \vdots \\
		   x^{(N)} \\
         \end{pmatrix}
  \end{align}

In unsupervised learning, however, the problem has no output, that is, all examples come only with the input data. Therefore, one of the main tasks assigned to these types of algorithms is clustering, where the algorithm learns to find patterns in the input data to separate them into groups called clusters. 

Part III-(A) of this book will explain supervised learning algorithms, whereas Part III-(B) will explain unsupervised learning algorithms.

\bibliographystyle{unsrt}
\bibliography{bibliography}

%\end{document}
