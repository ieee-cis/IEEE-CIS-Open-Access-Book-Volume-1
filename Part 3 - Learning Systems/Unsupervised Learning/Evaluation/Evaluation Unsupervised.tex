\title{Clustering Evaluation}
\label{chp:evaluation-unsupervised-learning}
\author{Valmir Macario}
\institute{}
\maketitle
 
%\chapter{Optimization}
The evaluation of unsupervised algorithms, such as clustering methods, is aimed at validating the data structures (in the feature space) found by these algorithms. If the purpose is to validate newly developed algorithms, i.e., comparing them with others in the state of the art, it can be useful to compare the obtained results against previously known structures, and, then, assessing the capacity of these algorithms to find a structure close to the one already known \cite{carvalho2011inteligencia}. However, in some cases a previously known structure may not be available. So it is necessary to validate whether the found structure met the expected criteria for optimal clustering structures. Some aspects that are inherent of an optimal scheme are the two proposed criteria for clustering evaluation and selection \cite{Berry1996}:
\begin{itemize}
\item Compactness: the members of the same cluster should be as close to each other as possible. A common measure of compactness is the variance, which must be minimized.

\item Separation: different clusters must be be widely spaced. There are three common approaches measuring the distance between two different clusters \cite{halkidi2001clustering}:
\begin{itemize}
    
\item Single linkage: it measures the distance between the closest members of the clusters.
\item Complete linkage: it measures the distance between the most distant members.
\item Comparison of centroids: it measures the distance among the centers of the clusters.
\end{itemize}
\end{itemize}

These peculiarities make the clustering validation a complex task, once there is no a known solution to compare the results obtained by the algorithms, and often there is no a single solution. 

The validation of a clustering performance is, in general, based on statistical measures, which evaluate in a quantitative and objective perspectives the found structures \cite{jain1988algorithms}. The index that is used to validate structures is called validation criterion. Cluster evaluation criteria strategies are usually divided in three types:
\begin{itemize}
\item Relative criteria: evaluate clusters based on a criterion that assesses whether the performance yields good clustering partitions according to some assumptions. They can be used to compare multiple clustering algorithms or to determine the most appropriate value for one or more parameters of an algorithm, such as the number of clusters.

\item Internal criteria: only use descriptive characteristics of the original data to validate the quality of a cluster.

\item External criteria: evaluate the quality of a clustering performance according to previously known structures. These structures are usually labeled datasets, which are used to assess the juxtaposition among the clusters generated by the algorithm and the previously labeled structure.
\end{itemize}

These validation criteria can be used to evaluate several types of clustering structures such as hierarchies, partitions (hard or fuzzy) and individual clusters.

Usually a relative criterion is used to compare multiple clustering performances. The best option is determined by the maximum or minimum values for a specific index, because for some validation criteria a higher value denotes a best clustering performance, while for other validation criteria the minimum value is related to the best clustering performance.

The external and internal validation criteria are based on statistical tests and have a high computational cost. \cite{halkidi2001clustering}. The basic idea is to test whether the examples of a dataset are randomly structured or not. This analysis is based on the Null Hypothesis, $H0$, expressed as a statement of random structure of a dataset. To test this hypothesis, the Monte Carlo techniques are often used as a solution \cite{Theodoridis2009}. However, it is very difficult to set thresholds for deciding whether the index value is large or small enough to consider the resulting clusters potentially useful or valid \cite{carvalho2011inteligencia}. So, a number of validity indices have been defined and proposed in the literature for each of above approaches \cite{jain1988algorithms,halkidi2002cluster,halkidi2002clustering,Theodoridis2009}. In the next section some classic relative, external and internal criteria index are presented.


\section{Relative Criteria}
\label{sec:relative_criterian}

The relative criteria evaluate the partitions $\pi$ according to a scheme of a set of defined schemes according to a pre-specified criterion. The best clustering is selected based on the capability of a relative measure, like compactness and separation of the partitions, to mirror the behavior of the external index and properly distinguish among better and worse partitions \cite{vendramin2010relative}.


\subsection{Mathematical Notation}

Let $\mathcal{T} = \{\mathbf{x}^{(1)}, \mathbf{x}^{(2)},\ldots, \mathbf{x}^{(i)}, \ldots, \mathbf{x}^{(n)}\}$ be the dataset which consists of $n$ observed examples of the form $\mathbf{x}^{(i)} = \{x^{(i)}_1 ,x^{(i)}_{2},\ldots,x^{(i)}_{p}\}$ where $p$ stands as the number of features of the problem. Be $\pi^e=\{\mathbf{c}^{(1)}, \mathbf{c}^{(2)}, \ldots, \mathbf{c}^{(k)}\}$ (e.g., the set of resulting clusters' representations) the clustering algorithm partition result, and $\pi^r=\{C_1, C_2, \ldots, C_k\}$ the ground truth on the clustering structure (i.e., $C_i$ comprises all examples in the cluster $i$). The cluster representation, that can be a centroid (i.e., the mean of the examples in a cluster) or a medoid (i.e., the example in $\mathcal{T}$ nearest to the centroid) of cluster $k$ is $\mathbf{c}^{(k)}$. In addition, it is also necessary to define a similarity or dissimilarity measure between two examples $\mathbf{x}^{(i)}$ and $\mathbf{x}^{(j)}$, i.e., $d(\mathbf{x}^{(i)},\mathbf{x}^{(j)})$. Typically the similarity measure used for most methods is the Euclidean distance (Equation \ref{eq:euclid_dist2}): 

% \begin{equation}
% \label{eq_euclidean}
% d(x_i,x_j)=||x_i-x_j||_2=\sqrt{\sum_{d=1}^p (x_{id}-x_{jd})^2}  
% \end{equation}

\begin{equation}
    d(\textbf{x}^{(i)},\textbf{x}^{(j)}) = \sqrt{(x^{(i)}_{1} - x^{(j)}_{1})^{2} + (x^{(i)}_{2} - x^{(j)}_{2})^{2} + \cdots + (x^{(i)}_p - x^{(j)}_{p})^{2}}
    \label{eq:euclid_dist2}
\end{equation}

\subsection{Intracluster Variance}

The intracluster variance is given by Equation~\eqref{eq_pi}, where $\mathbf{c}^{(k)}$ is the centroid of the cluster $C_k$ \cite{handl2005computational}. This measure assesses the quality of the clusters based on the compression among them. The output values are in the range $[0,\infty]$ so that the smaller the value of $var$, the better the partition. 
\begin{equation}
\label{eq_pi}
    var(\pi)=\sqrt{\frac{1}{n} \sum_{\mathbf{c}^{(k)} \in \pi^e} \sum_{\mathbf{x}^{(i)} \in C_k} d(\mathbf{x}^{(i)},\mathbf{c}^{(k)})}
\end{equation}

\subsection{Connectivity}

Connectivity reflects the degree to which neighboring examples are placed in the same cluster. Connectivity is given by Equation \ref{eq_con}, where $v$ is the number of closest neighbors that contribute to connectivity and $nn_{ij}$ is the $\mathbf{j}^{(th)}$ closest neighbor to the example $\mathbf{x}^{(i)}$. The smaller the $con$ index value, the better the partition. The values of this index vary from $[0,\infty]$.

\begin{equation}
\label{eq_con}
    con(\pi) = \sum_{\mathbf{x}^{(i)} \in \mathcal{T}} \sum_{j=1}^v f(\mathbf{x}^{(i)},nn_{ij})
\end{equation}

\begin{equation}
    f(\mathbf{x}^{(i)},nn_{ij})=\left\{\begin{matrix}
\frac{1}{j} & \;if\;\mathbf{x}^{(i)} \in \mathbf{c}^{(k)},\; nn_{ij} \notin \mathbf{c}^{(k)}\\ 
0 & if\;not 
\end{matrix}\right.
\end{equation}

\subsection{Dunn Index}

The Dunn's index \cite{halkidi2002clustering} goal is to identify compact and separate clusters. Thus, in the best scenario, the distance between two clusters is large and their diameters are small. It is a function of the distance between the clusters $C_a$ and $C_b$ so that $d(C_a)$ measures the internal distance dispersion of cluster $C_a$. 

\begin{equation}
\label{eq_dunn}
    D(\pi)=\min_{a=1,\ldots,k}\left\{\min_{b=a+1,\ldots,k} \left\{\frac{d(C_a,C_b)}{\max_{l=1,\ldots,k} d(C_l)}\right\}\right\}
\end{equation}

\begin{equation}
\label{eq_dunn_1}
    d(C_a,C_b)=\min_{\mathbf{x}^{(i)} \in Ca,\mathbf{x}^{(j)} \in C_b} d(\mathbf{x}^{(i)},\mathbf{x}^{(j)})
\end{equation}
    
\begin{equation}
\label{eq_dunn_2}
    d(C_a)=\max_{\mathbf{x}^{(i)},\mathbf{x}^{(j)} \in C_a} d(\mathbf{x}^{(i)},\mathbf{x}^{(j)})
\end{equation}

The Dunn's index is not influenced by the number of clusters. By evaluating the number of clusters from $x \geq 2$, the highest value of Dunn's index can indicate the optimal number of clusters, i.e., the number of clusters that better represents the data.

The problem with the Dunn's index is twofold: (i) its complexity and (ii) its sensitivity to noise examples which can cause an increase in the diameter of the clusters. To tackle its sensitivity to noise there are some variations of the index that mainly change the distances calculated in Equations \ref{eq_dunn_1} and \ref{eq_dunn_2} \cite{bezdek1998some}.

\subsection{Silhouette}

The silhouette value \cite{rousseeuw1987silhouettes} was pointed out in \cite{Brun07} as the most suitable relative index according to their experiments. Higher values of the silhouette indicates compact and separated clusters. 

\begin{equation}
    sil(\mathbf{x}^{(i)})=\left\{\begin{matrix}1-\frac{a(\mathbf{x}^{(i)},C_i)}{b(\mathbf{x}^{(i)})},
\\ 0,
\\ \frac{b(\mathbf{x}^{(i)})}{a(\mathbf{x}^{(i)},C_i)-1}
\end{matrix}\right.
\end{equation}

\begin{equation}
a(\mathbf{x}^{(i)},C_i)=\frac{1}{|C_k|}\sum_{\mathbf{x}^{(i)},\mathbf{x}^{(j)} \in C_k, \mathbf{x}^{(i)} \neq \mathbf{x}^{(j)}} d(\mathbf{x}^{(i)}, \mathbf{x}^{(j)})     
\end{equation}

\begin{equation}
    b(\mathbf{x}^{(i)})=\min_{\mathbf{x}^{(i)} \in C_i,
    C_i \neq C_j} a(\mathbf{x}^{(i)},C_j)
\end{equation}

\noindent
in which $a(\mathbf{x}^{(i)}, C_k)$ is the average distance between $\mathbf{x}^{(i)} \in C_k$ and the remaining examples in $C_k$ and $b(\mathbf{x}^{(i)})$ is the minimum of the average distances between $\mathbf{x}^{(i)}$ and examples belonging to different clusters. The value of $sil(\mathbf{x}^{(i)})$ ranges from $-1$ to $+1$. If the value is close to $-1$, the example $\mathbf{x}^{(i)}$ is closer, on average, to a cluster $C_j$ such that $\mathbf{x}^{(i)} \notin C_j$. If the value is close to $+1$, then it means that average distance of example $\mathbf{x}^{(i)}$ to the cluster it belongs to is smaller than to any different cluster.

Besides the silhouette of each example, the silhouette of each cluster can be computed according to Equation \ref{eq_sil_C}. One way to choose the best value of $k$ is to select the value that yields the greatest value of $sil(\pi)$.


\begin{equation}
\label{eq_sil_C}
    sil(C_k) = \frac{1}{|C_k|}\sum_{\mathbf{x}^{(i)} \in C_k} sil(\mathbf{x}^{(i)})
\end{equation}

\begin{equation}
    sil(\pi) = \frac{1}{n}\sum_{i = 1}^n sil(\mathbf{x}^{(i)})
\end{equation}


The Silhouette Coefficient (SC), is the maximum $sil(\pi)$ for $\pi$ generated with $k=2,3,\ldots, (n-1)$. SC is a measure that quantifies the structure discovered by a clustering algorithm. A value close to $0$ means that no substantial structure was found. Less than $0.5$ indicates that the structure is weak, between $0.5$ and $0.7$ indicates a reasonable structure and greater than $0.7$ indicates a strong structure \cite{carvalho2011inteligencia}.

It is worth to notice the existence of another type of clustering algorithms that create fuzzy partitions of the dataset. The fuzzy clustering algorithms yields a pertinence degree $u_{ik}$ of each example to each cluster, rather than being associated with just one cluster as in a hard partition. There are several indexes used for fuzzy clustering algorithms: partition coefficient (PC) \cite{pal1995cluster} , partition entropy (PE) \cite{pal1995cluster}, Xie-Beni index (XB) \cite{pal1995cluster} extended Xie-Beni index \cite{pal1995cluster}, Fukuyama-Sugeno (FS) \cite{pal1995cluster} and others \cite{pal1995cluster,pakhira2004validity}.


\section{Internal Criteria}
\label{sec:intern_criterian}

Internal criteria indexes measure the match of a cluster obtained by the algorithm to the clustering structure without having access to external information about the ground truth on the clustering structure (see next Subsection \ref{sec_external_index}). In general, these criteria are used to select the best number of clusters $k$. Normally, two types of internal validation metrics can be combined: cohesion and separation measures. Cohesion is an intra-cluster measure that evaluates how closely the examples of the same cluster are to each other, while separation is an inter-cluster measure that quantifies the level of separation between clusters \cite{palacio2019evaluation}. Cohesion can be computed by Equation \ref{eq_cohesion} and separation by Equation \ref{eq_separation}.
\begin{equation}
\label{eq_cohesion}
    Cohesion(C_k)=\sum_{\mathbf{x}^{(i)} \in C_k} \sum_{\mathbf{x}^{(j)} \in C_k} d(\mathbf{x}^{(i)},\mathbf{x}^{(j)})
\end{equation}

\begin{equation}
\label{eq_separation}
    Separation(C_k, C_l)=\sum_{\mathbf{c}^{(i)} \in C_k} \sum_{\mathbf{c}^{(j)} \in C_l} d(\mathbf{c}^{(i)},\mathbf{c}^{(j)})
\end{equation}

These metrics can also be defined for prototype-based clustering techniques, where the similarities from data examples to cluster centroids or medoids are measured. When the similarity function is the squared Euclidean distance, the cohesion metric defined above is equivalent to the cluster SSE (Sum of Squared Errors); also known as SSW (Sum of Squared Errors Within Cluster) \cite{palacio2019evaluation}.  

\begin{equation}
\label{eq_sse}
    SSE=\sum_{\mathbf{x}^{(i)} \in C_k} d(\mathbf{x}^{(i)}, \mathbf{c}^{(k)})^2 = \frac{1}{2m_i} \sum_{\mathbf{x}^{(i)} \in C_k} \sum_{\mathbf{x}^{(j)} \in C_k} d(\mathbf{x}^{(i)},\mathbf{x}^{(j)})^2
\end{equation}

There are some difficulties when using these measures, like the $SSE$ that presents lower values for data that actually have clusters than for random, unstructured data. Another difficulty is the dependence of these indexes on the values used for data characteristics, such as number of objects, number of dimensions, number of clusters and scattering \cite{jain1988algorithms}. To alleviate some of these limitations, the relative indexes presented in Section \ref{sec:relative_criterian} also can be used as internal criteria. Besides the relative indexes, you can use some others indexes, like the Gap Statitical \cite{tibshirani2001estimating} and Clest proceeding \cite{dudoit2002prediction} to compute internal criteria. 


\iffalse
In addition to them, we can use the Gap criterion that will be explained in the next section.
\subsection{Estatística Gap}
O índice Gap \cite{Tibshirani2001b} calculado pela Equação \ref{eq_gap1}, possui um valor chamado de Esperança $W_k$ calculado pela Equação \ref{eq_gap2} utilizado pra o cálculo da dispersão da distribuição intracluster em relação a essa esperança. A variável $D$ calculado pela Equação \ref{eq_gap3} representa a disperção intracluster. 

\begin{equation}
\label{eq_gap1}
    Gap_n(k)=E_n\{\log(W_k)\}-\log(W_k)
\end{equation}

\begin{equation}
\label{eq_gap2}
W_k=\sum_{r=1}^k \frac{1}{2n_r}D_r    
\end{equation}

\begin{equation}
\label{eq_gap3}
    D_r=\sum_{x_i,x_j \in C_r} d(x_i,x_j)
\end{equation}

Esse índice calcula o número ótimo de clusters obtendo o valor de $k$ para o qual $log(W_k)$ se situa o mais abaixo possível da curva de referência. A curva de referência pode ser obtida de duas formas:

\begin{itemize}
    \item[a] Gerar cada atributo de referência uniformemente no intervalo de valores observados para aquele atributo ou
    \item[b] Gerar atributos de referência a partir de uma distribuição uniforme sobre um retângulo alinhado com os componentes principais dos dados.
\end{itemize}

O método a é mais simples, porém o métdodo b leva em consideração a forma da distribuição dos dados e torna o procedimento a invariantes à rotação, se o método de agrupamento também for invariante. 

Para calcular a estatística Gap, estima-se $E_n=log(W_k)$ pela média de $B$ cópias $log(W_k)$, cada uma computada de uma mostra de Monte Carlo extraída da distribuião de referência. Ems seguida, é preciso avaliar a distribuição amostral da estatística Gap. O Algoritmo \ref{algo_gap} mostra o processo do cálculo da estatística Gap.
 
%%%%%%%%%%%%%%%%%%%%%%%% ALGORITHM
\begin{algorithm}[ht!]
\label{algo_gap}
	%\renewcommand{\baselinestretch}{0.8}
	\caption{Statistic Gap Computation}
    
    Parameters: Dataset X, Clustering Algorithm $\phi$
    
    Output: Gap Statistic. Best number of clusters.
    
	\begin{algorithmic}[1] 
		\STATE{Generate $B$ sets of references $X_b$, using methods a or b}, described before
		\FOR{each number of clusters $k=1,\ldots,K$}  \DO
        \STATE{Execute the algorithm $\phi$ using reference dataset $X$, generating the partition $\pi^k$}
		\STATE{Compute the dispersion metric $W_{k}$ for the partition $\pi^{k}$}
		\FOR{reference dataset $X_b$} \DO 
		\STATE{Execute the algorithm $\phi$ using reference dataset $X_b$, generating the partition $\pi^kb$}
		\STATE{Compute the dispersion metric $W_{kb}$ for the partition $\pi^{kb}$}
		\ENDFOR
		\ENDFOR
		
		\STATE{Compute the Gap Statistic using Equation \ref{eq_gapalgo1}
		 \begin{equation}
		    \label{eq_gapalgo1}
		     Gap_n(k)=\frac{1}{B}\sum_{b=1}^B \log(W_{kb}- \log(W_k) 
		\end{equation}
		
		\STATE{Compute the standard deviation $sd_k$, using Equation \ref{eq_gapalgo2} with $\bar{l}=(1/B)\sum_b \log(W_{kb})$
		 \begin{equation}
		    \label{eq_gapalgo2}
		     sd_k=\sqrt{\frac{1}{B}\sum_{b=1}^B \log(W_{kb})- \bar{l}}
		\end{equation}
	\STATE{Set $s_k=sd_k\sqrt{1+1/B}$}
	\STATE{Choose the best number of cluster as the minor value of $k$ that $Gap(k) \geq Gap(k+1)-s_{k+1}$}
	\end{algorithmic}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%% ALGORITHM
O cálculo da estatística Gap assime que já clusters uniformes separados. Quando existem subclusters menores dentro de clusters maiores bem separados, a estatística Gap pode exibir um comportamento não monotônicos. Assim, importante examinar a curva Gap inteira, em vez de simplesmente buscar o seu maximo. 
\fi

\section{External Index}
\label{sec_external_index}

External validation metrics compare the resulting partitions ($\pi^e$) achieved by a clustering method and independent partitions created based on prior knowledge about the real data structure ($\pi^r$), usually performed on labeled data. Since unsupervised learning techniques are primarily used when class labels are not available, external validation methods can't be applied on most of the clustering problems. However, they can still be applied when external information is available (e.g., for synthetic data)\cite{palacio2019evaluation}. 

An external index for partitions suitability evaluates the degree to which two partitions of $n$ examples match. In order to carry out this analysis, a contingency matrix must be built to evaluate the clusters found by the algorithm. Given a pair of examples $(\mathbf{x}^{(i)},\mathbf{x}^{(j)})$, the contingency matrix can depict four possible situations:
\begin{itemize}
    \item True Positive (TP): they belong to the same cluster, both in $\pi^e$ and in $\pi^r$.
    \item False Positive (FP): they belong to the same cluster in $\pi^e$ but belong to different clusters in $\pi^r$.
    \item False Negative (FN): they belong to different clusters in $\pi^e$ but belong to the same cluster in $\pi^r$.
    \item True Negative (TN): they belong to different clusters, both in $\pi^e$ and $\pi^r$.
\end{itemize}

From these four situations, we can easily obtain:

\begin{itemize}
\item The number of pairs belonging to the same cluster in $\pi^e$: $m1 = TP + FP.$
\item The number of pairs belonging to the same cluster in $\pi^r$: $m2 = TP + FN.$
\item The total number of pairs of examples: $M = TP + FP + FN + TN = \frac{n(n-1)}{2}$
\end{itemize}

These values can then be used to compute a number of external indexes, such as Hubbert, Jaccard, Rand and Corrected Rand \cite{jain1988algorithms} that will be described in next subsections.
 
\subsection{Rand Index}

The Rand index, computed by Equation \ref{eq_rand}, measures the probability of two examples to belong to same cluster or to belong to different clusters on both partitions $\pi^e$ and $\pi^r$.

\begin{equation}
\label{eq_rand}
    Rand=\frac{TP+TN}{M}
\end{equation}


\subsection{Jaccard Index}
\label{eq_jaccard}

The Jaccard index computes the probability of two examples that belong to same cluster on one of the partitions also belong to the same cluster on the other partition. The Jaccard index is defined by Equation \ref{eq_jaccard}.

\begin{equation}
    J=\frac{TP}{(TP+FP+FN)}
\end{equation}
    
\subsection{Folkes and Mallows Index}
\label{sec_folkes}

The Folkes and Mallows index computes the similarity among the clusters found by the algorithm with respect to the a ground-truth. This index ranges between 0 and 1 and larger values indicate more similarity between two partitions. This index is computed by Equation \ref{eq_folkes}. 

\begin{equation}
\label{eq_folkes}
    FM=\sqrt{\frac{TP}{(TP+FP)}\times\frac{TP}{(TP+FN)}}
\end{equation}


\subsection{Normalized Hubert Index}
\label{eq_hubert}

The Normalized Hubert Index measures the linear correspondence between two partitions. The values are in the range [-1,1] with bigger values indicating that the two partitions have bigger correspondence. Equation \ref{eq_hubert} defines the Normalized Hubert Index.

\begin{equation}
    \Gamma=\frac{(M\times TP)-(m_1m_2)}{\sqrt{m_1m_2(M-m_1)(M-m_2)}}
\end{equation}


\subsection{Corrected Rand Index}

A common characteristic for most of external indexes is their high sensitivity to the number of classes in partitions or to the spatial distribution of examples in clusters. For example, some indexes tend to output higher values for partitions with more classes (Hubbert and Rand), others for partitions with fewer classes (Jaccard) \cite{Dubes1987}. The Corrected Rand Index (CR) has its outputs corrected according to correct answers in the comparisons of the partitions, so it does not have any of these undesirable characteristics \cite{Milligan1986}. This index can be obtained from the contingency table and built from the two partitions shown in Table \ref{tab_contigencia}. The $CR$ can be defined by Equation \ref{eq_CR}:

\begin{table}[h]
\begin{center}
\caption{Contingency table of two partitions}
\label{tab_contigencia}
\begin{tabular}{|c|c|c|c|c|c|}
\hline &  $C_1$ & $C_2$ & \ldots & $C_K$\ & \\
\hline $Q_1$  & $n_{11}$ & $n_{12}$ & ...   & $n_{1K}$ & $n_{1.}$ \\
  $Q_2$  & $n_{21}$ & $n_{22}$ & ...   & $n_{2K}$ & $n_{2.}$ \\
 ... & ... & ... &   & ... & \\
 $Q_H$  & $n_{H1}$ & $n_{H2}$ & ...  & $n_{HK}$ & $n_{R.}$ \\
\hline  & $n_{.1}$ & $n_{.2}$ &       & $n_{.K}$ & $n_{..}=n$ \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{eqnarray}\label{eq_CR}
CR=
\frac{\sum_h^H \sum_k^K \binom{n_{hk}}{2}-\binom{n}{2}^{-1} \sum_h^H \binom{n_{h\cdot}}{2} \sum_k^K \binom{n_{\cdot k}}{2}}
{\frac{1}{2} \left[\sum_h^H \binom{n_{h\cdot}}{2} + \sum_k^K \binom{n_{\cdot k}}{2}\right] - \binom{n}{2}^{-1} \sum_h^H \binom{n_{h\cdot}}{2} \sum_k^K \binom{n_{\cdot k}}{2}}
\end{eqnarray}

\noindent
where $\binom{n}{2}=\frac{n(n-1)}{2}$ and $n_{hk}$ is the number of examples belonging to the same cluster for both partitions $\pi^e_k$ and $\pi^r_h$. The term $n_{\cdot k}$ depicts the number of examples in cluster $C_k$ and $n_{h\cdot}$ is the number of examples at class $Q_h$, and $n$ is the total number of examples.

This index varies in the range [-1, 1], where 1 indicates perfect cohesion between the partitions and -1 indicates that there is no cohesion between the partitions. Milligan and Cooper's work \cite{Milligan1986} indicates that values below 0.05 are randomly generated partitions.

\subsection{Information Variation Index}

The information variation (VI) \cite{carvalho2011inteligencia} is computed according to Equation \ref{eq_vi}. It measures the amount of information lost or gained to move from the $\pi^e$ partition to the $\pi^r$ partition. In this equation, $EN(\pi^a)$ computes the entropy of the partition $\pi^a$ using Equation \ref{eq_entropy} and the mutual information shared between two partitions is given by the Equation \ref{eq_mutual}. The probability of an object share both partitions $\pi^e$ and $\pi^r$ is given by $p(k,h)=\frac{n_{hk}}{n}$ and the probability of an object belong to cluster $C_k$ is $p(k)=\frac{n_{\cdot k}}{n}$. This index has zero as the best value, indicating that the two partitions are identical and have no upper limit.

\begin{equation}
\label{eq_mutual}
I(\pi^a,\pi^b)=\sum_{k=1}^K \sum_{h=1}^H p(k,h) \log\left(\frac{p(k,h)}{p(k)p(h)} \right)
\end{equation}

\begin{equation}
\label{eq_entropy}
EN(\pi^a)=-\sum_{k=1}^K p(k) \log p(k)
\end{equation}
\begin{equation}
\label{eq_vi}
VI(\pi^e,\pi^r)=EN(\pi^e)+EN(\pi^r)-2I(\pi^r,\pi^r)
\end{equation}

\subsection{Normalized Mutual Information Index}
The external mutual information index ($I$) originates from the principles of information theory and the notion of entropy \cite{Cover2006}. When entropy is applied to the clustering process, it is a metric that measures the uncertainty that an element $\mathbf{x}^{(i)}$ will be associated with a certain cluster $C_k$. For the case that an element is associated with a cluster by chance, the entropy will be 0. The notion of entropy can be extended to mutual information, which measures how much we can, on average, reduce the uncertainty about the random choice that an element $\mathbf{x}^{(i)}$ is associated with a $C_k$ cluster, taking into account that its cluster is known from another previous clustering. The mutual information is calculated using the Equation \eqref{eq_mutual}.

The mutual information $I$ is a metric about the space of all clusters. However, it is not limited by a constant value, which makes it difficult to interpret. In the work by Strehl and Ghosh \cite{Strehl2002}, a normalization by geometric mean was proposed. To this end, they determine the cluster that has the maximum average of the normalized mutual information of all the clusters under consideration, where the normalized mutual information (NMI) between two clusters is defined as follows by the Equation \ref{eq_NMI_1}.
\begin{eqnarray}
\label{eq_NMI_1}
NMI(\pi^e,\pi^r)=\frac{I(\pi^r,\pi^e)}{\sqrt{EN(\pi^r)EN(\pi^e)}}
\end{eqnarray}
\noindent
where $I(\pi^r,\pi^e)$ is the mutual information between $\pi^r$ and $\pi^e$. $EN(\pi^r)$ and $EN(\pi^e)$ are entropies of $\pi^r$ e $\pi^r$ respectively computer with Equation \ref{eq_entropy}. NMI has its values between $[0,1]$, being 1 the best clustering match.



%Thereby, the normalized mutual information can be %computer with Equation \ref{eq_NMI}.

%\begin{eqnarray}
%\label{eq_NMI}
%NMI=\frac{\sum_{k=1}^K \sum_{h=1}^H n_{hk} %\log\left(\frac{n\,n_{hk}}{n_{\cdot k} %n_{h\cdot}}\right)}{\sqrt{\left(\sum_{k=1}^K %n_{\cdot k}\log \frac{n_{\cdot k}}{n} \right) %\left( \sum_{h=1}^H n_{h\cdot} %\log\frac{n_{h\cdot}}{n}\right)}}
%\end{eqnarray}
%\noindent
%where $n_{hk}$, $n_{\cdot k}$ eand$n_{h\cdot}$ are %as described. NMI has its values between the %interval [0,1].

\subsection{Accuracy Rate}
The accuracy (ACC) looks for a decision rule that minimizes the probability of error \cite{Breiman1984}. The maximum value of a ACC indicates the best performance of the clustering algorithm.

\begin{eqnarray}
\label{eq_ERC}
%ERC=\sum_{k=1}^K \frac{n_{\cdot k}}{n}\left(1-\max_{1 \leq h \leq H} \left(\frac{n_{hk}}{n_{\cdot k}}\right)\right)
ACC = \left(\frac{\sum_{k=1}^K \max_{1 \leq h \leq H} \,n_{hk}}{n}\right)
\end{eqnarray}
\noindent
where $n_{hk}$ is as described.


\section{Summary and Discussion}
In this chapter, strategies and evaluation criteria for clustering algorithms analysis were disclosed.
Briefly, the three types of existing criteria for evaluation were detailed: relative, internal and external criteria. The relative criterion is useful to compare several clustering strategies with respect to some aspect and to decide the ideal number of clusters. The internal criteria, evaluate the quality of a cluster according to some property existing in the original data. The relative and internal criteria evaluate cluster quality when there is no previous information about the original data. Yet, the external criteria allows the confrontation of the clustering obtained by an algorithm with a previously known data structure.
In addition, several indices that can be applied with each criterion were discussed, different ways in which these indices can be used,
as well as several recent approaches that consider aspects such as stability of generated partitions.


\section{Exercises}


\begin{enumerate}
    \item What is the difference between an external and relative criteria index?

    \item Compare Dunn's and Silhouette's indices. Which clustering criteria do they benefit?

    \item An expert is analyzing the result of a clustering algorithm on a dataset $A$. He knows that this set of data has a known structure $\pi^r$. In the experiments were obtained the following values: $TP=92$, $FP=8$, $FN=12$ and $TN=88$. Compute the external indices: 

    \begin{enumerate}
        \item Rand Index 

        \item Jaccard Index 

        \item Folkes and Mallows Index 

        \item Normalized Hubert Index. 
        
    \end{enumerate}

    \item A comparison with 2 clustering algorithms has obtained the best partitions of each algorithm ($\pi^1,\pi^2$), respectively. Computing the corrected rand index for each partition were obtained the following values: $CR(\pi^1, \pi^r)=0,9$ and $CR(\pi^2, \pi^r)=0,2$. What you can say about each of partitions?
    
\end{enumerate}


\iffalse
All the external indexes above can also be used in fuzzy clustering algorithms by transforming the fuzzy partition in a hard partition. Let $F=(F_1,\ldots,F_K)$ of $X$ in $K$ fuzzy clusters represented by $\textbf{U}=(\textbf{u}_1,\ldots,\textbf{u}_n)$, with $\textbf{u}_{ik} = (u_{i1},\ldots,u_{iK}) \, (i=1,\ldots,n)$. So, a hard partition $C = (C_1, \ldots,C_K)$ will be fetched from that fuzzy partition by defining a partition $C_k (k = 1,\ldots,K)$ with $C_k = \{x_i : u_{ik} \ge u_{il} \, \forall l \in \{1,\ldots,K\}\}$.
\fi
%FAlta sessão final


\bibliographystyle{unsrt}
\bibliography{bibliography}
